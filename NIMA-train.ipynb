{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8aa2f1ba-be3f-46b3-8f28-6068685e8a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean dataset\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torchvision.datasets.folder import default_loader\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "# logger = logging.getLogger(__file__)\n",
    "\n",
    "\n",
    "def _remove_all_not_found_image(df: pd.DataFrame, path_to_images: Path) -> pd.DataFrame:\n",
    "    clean_rows = []\n",
    "    for _, row in df.iterrows():\n",
    "        image_id = row[\"image_id\"]\n",
    "        try:\n",
    "            file_name = f\"{path_to_images}/{image_id}.jpg\"\n",
    "            _ = default_loader(file_name)\n",
    "        except (FileNotFoundError, OSError, UnboundLocalError) as ex:\n",
    "            print(f\"broken image {file_name} : {ex}\")\n",
    "        else:\n",
    "            clean_rows.append(row)\n",
    "    df_clean = pd.DataFrame(clean_rows)\n",
    "    return df_clean\n",
    "\n",
    "\n",
    "def remove_all_not_found_image(df: pd.DataFrame, path_to_images: Path, num_workers: int) -> pd.DataFrame:\n",
    "    futures = []\n",
    "    results = []\n",
    "    with ThreadPoolExecutor(max_workers=num_workers) as executor:\n",
    "        for df_batch in np.array_split(df, num_workers):\n",
    "            future = executor.submit(_remove_all_not_found_image, df=df_batch, path_to_images=path_to_images)\n",
    "            futures.append(future)\n",
    "        for future in tqdm(as_completed(futures), total=len(futures)):\n",
    "            results.append(future.result())\n",
    "    new_df = pd.concat(results)\n",
    "    return new_df\n",
    "\n",
    "\n",
    "def read_ava_txt(path_to_ava: Path) -> pd.DataFrame:\n",
    "    # NIMA origin file format and indexes\n",
    "    df = pd.read_csv(path_to_ava, header=None, sep=\" \")\n",
    "    del df[0]\n",
    "    score_first_column = 2\n",
    "    score_last_column = 12\n",
    "    tag_first_column = 1\n",
    "    tag_last_column = 4\n",
    "    score_names = [f\"score{i}\" for i in range(score_first_column, score_last_column)]\n",
    "    tag_names = [f\"tag{i}\" for i in range(tag_first_column, tag_last_column)]\n",
    "    df.columns = [\"image_id\"] + score_names + tag_names\n",
    "    # leave only score columns\n",
    "    df = df[[\"image_id\"] + score_names]\n",
    "    return df\n",
    "\n",
    "\n",
    "def clean_and_split(\n",
    "    path_to_ava_txt: Path, path_to_save_csv: Path, path_to_images: Path, train_size: float, num_workers: int\n",
    "):\n",
    "    print(\"read ava txt\")\n",
    "    df = read_ava_txt(path_to_ava_txt)\n",
    "    print(\"removing broken images\")\n",
    "    df = remove_all_not_found_image(df, path_to_images, num_workers=num_workers)\n",
    "    print(\"train val test split\")\n",
    "    df_train, df_val_test = train_test_split(df, train_size=train_size)\n",
    "    df_val, df_test = train_test_split(df_val_test, train_size=0.5)\n",
    "    train_path = f\"{path_to_save_csv}/train.csv\"\n",
    "    val_path = f\"{path_to_save_csv}/val.csv\"\n",
    "    test_path = f\"{path_to_save_csv}/test.csv\"\n",
    "    print(f\"saving to {train_path} {val_path} and {test_path}\")\n",
    "    df_train.to_csv(train_path, index=False)\n",
    "    df_val.to_csv(val_path, index=False)\n",
    "    df_test.to_csv(test_path, index=False)\n",
    "\n",
    "\n",
    "path_to_ava_txt = \".../AVA.txt\"\n",
    "path_to_save_csv = \".../dataset_csv\"\n",
    "path_to_images = \".../image\"\n",
    "train_size = 0.8\n",
    "num_workers = 12\n",
    "\n",
    "print(f\"Clean and split dataset to train|val|test in {num_workers} threads. It will takes several minutes\")\n",
    "clean_and_split(\n",
    "    path_to_ava_txt=path_to_ava_txt,\n",
    "    path_to_save_csv=path_to_save_csv,\n",
    "    path_to_images=path_to_images,\n",
    "    train_size=train_size,\n",
    "    num_workers=num_workers,\n",
    ")\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "59db887c-b7e8-43aa-9290-287db003bf29",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_NET_MEAN = [0.485, 0.456, 0.406]\n",
    "IMAGE_NET_STD = [0.229, 0.224, 0.225]\n",
    "\n",
    "from torchvision import transforms\n",
    "\n",
    "class SquarePad:\n",
    "    def __call__(self, image):\n",
    "        w, h = image.size\n",
    "        max_wh = np.max([w, h])\n",
    "        hp = int((max_wh - w) / 2)\n",
    "        vp = int((max_wh - h) / 2)\n",
    "        padding = (hp, vp, hp, vp)\n",
    "        return transforms.functional.pad(image, padding, 0, 'constant')\n",
    "    \n",
    "class Transform:\n",
    "    def __init__(self):\n",
    "        normalize = transforms.Normalize(mean=IMAGE_NET_MEAN, std=IMAGE_NET_STD)\n",
    "        \n",
    "        self._train_transform = transforms.Compose(\n",
    "            [\n",
    "                # SquarePad(),\n",
    "                transforms.Resize((256, 256)),\n",
    "                transforms.RandomHorizontalFlip(),\n",
    "                transforms.RandomCrop((224, 224)),\n",
    "                transforms.ToTensor(),\n",
    "                normalize,\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        '''\n",
    "        \n",
    "        # inception\n",
    "        self._train_transform = transforms.Compose(\n",
    "            [\n",
    "                transforms.Resize((350, 350)),\n",
    "                transforms.RandomHorizontalFlip(),\n",
    "                transforms.RandomCrop((299, 299)),\n",
    "                transforms.ToTensor(),\n",
    "                normalize,\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        self._val_transform = transforms.Compose([transforms.Resize((299, 299)), transforms.ToTensor(), normalize])\n",
    "        '''\n",
    "        self._val_transform = transforms.Compose([transforms.Resize((224, 224)), transforms.ToTensor(), normalize])\n",
    "        # self._val_transform = transforms.Compose([SquarePad(), transforms.Resize((224, 224)), transforms.ToTensor(), normalize])\n",
    "\n",
    "    @property\n",
    "    def train_transform(self):\n",
    "        return self._train_transform\n",
    "\n",
    "    @property\n",
    "    def val_transform(self):\n",
    "        return self._val_transform\n",
    "\n",
    "\n",
    "class AverageMeter(object):\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "\n",
    "def set_up_seed(seed=42):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "390a5620-487c-4d45-9624-750015fb4bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from typing import Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision.datasets.folder import default_loader\n",
    "\n",
    "\n",
    "class AVADataset(Dataset):\n",
    "    def __init__(self, path_to_csv: Path, images_path: Path, transform):\n",
    "        self.df = pd.read_csv(path_to_csv)\n",
    "        self.images_path = images_path\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return self.df.shape[0]\n",
    "\n",
    "    def __getitem__(self, item: int) -> Tuple[torch.Tensor, np.ndarray]:\n",
    "        row = self.df.iloc[item]\n",
    "\n",
    "        image_id = row[\"image_id\"]\n",
    "        image_path = f\"{self.images_path}/{image_id}.jpg\"\n",
    "        image = default_loader(image_path)\n",
    "        x = self.transform(image)\n",
    "\n",
    "        y = row[1:].values.astype(\"float32\")\n",
    "        p = y / y.sum()\n",
    "\n",
    "        return x, p\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "11ef8560-3e5d-47a9-bb91-5f505f90209b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class EDMLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(EDMLoss, self).__init__()\n",
    "\n",
    "    def forward(self, p_target: torch.Tensor, p_estimate: torch.Tensor):\n",
    "        assert p_target.shape == p_estimate.shape\n",
    "        # cdf for values [1, 2, ..., 10]\n",
    "        cdf_target = torch.cumsum(p_target, dim=1)\n",
    "        # cdf for values [1, 2, ..., 10]\n",
    "        cdf_estimate = torch.cumsum(p_estimate, dim=1)\n",
    "        cdf_diff = cdf_estimate - cdf_target\n",
    "        samplewise_emd = torch.sqrt(torch.mean(torch.pow(torch.abs(cdf_diff), 2)))\n",
    "        return samplewise_emd.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0fc3dd9f-2dcb-4baa-a895-12f223921ac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BinaryLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BinaryLoss, self).__init__()\n",
    "\n",
    "    def forward(self, p_target: torch.Tensor, p_estimate: torch.Tensor):\n",
    "        target = get_mean_score(p_target)\n",
    "        estimate = get_mean_score(p_estimate)\n",
    "        total_loss = 0\n",
    "        batch_size = len(target)\n",
    "        for i in range(batch_size):\n",
    "            if min(target[i], estimate[i]) >= 5 or max(target[i], estimate[i]) <= 5:\n",
    "                total_loss += 1\n",
    "        return total_loss/batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "91df9160-9bd6-478b-8d69-d92e52485218",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BinaryLoss5(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BinaryLoss5, self).__init__()\n",
    "\n",
    "    def forward(self, p_target: torch.Tensor, p_estimate: torch.Tensor):\n",
    "        target = get_mean_score(p_target)\n",
    "        estimate = get_mean_score(p_estimate)\n",
    "        total_loss = 0\n",
    "        batch_size = len(target)\n",
    "        for i in range(batch_size):\n",
    "            if min(target[i], estimate[i]) >= 5.5 or max(target[i], estimate[i]) <= 5.5:\n",
    "                total_loss += 1\n",
    "        return total_loss/batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ef03a908-e547-4473-8a98-4bcafa80ed5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pretrainedmodels\n",
    "import torch.nn as nn\n",
    "import torchvision as tv\n",
    "\n",
    "\n",
    "MODELS = {\n",
    "    \"resnet18\": (tv.models.resnet18, 512),\n",
    "    \"resnet34\": (tv.models.resnet34, 512),\n",
    "    \"resnet50\": (tv.models.resnet50, 2048),\n",
    "    \"resnet101\": (tv.models.resnet101, 2048),\n",
    "    \"resnet152\": (tv.models.resnet152, 2048),\n",
    "    \"mobilenet-v2\": (tv.models.mobilenet_v2, 1280),\n",
    "    \"mobilenet-v3-small\": (tv.models.mobilenet_v3_small, 576),\n",
    "    \"inception-v3\": (tv.models.inception_v3, 2048),\n",
    "    \"nasnetmobile\": (pretrainedmodels.__dict__['nasnetamobile'], 1056)\n",
    "}\n",
    "\n",
    "\n",
    "class NIMA(nn.Module):\n",
    "    def __init__(self, base_model: nn.Module, input_features: int, drop_out: float):\n",
    "        super(NIMA, self).__init__()\n",
    "        self.base_model = base_model\n",
    "\n",
    "        self.head = nn.Sequential(\n",
    "            nn.ReLU(inplace=True), nn.Dropout(p=drop_out), nn.Linear(input_features, 10), nn.Softmax(dim=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.base_model(x)\n",
    "        # x = nn.functional.adaptive_avg_pool2d(x, (1, 1))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.head(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "def create_model(model_type: str, drop_out: float) -> NIMA:\n",
    "    create_function, input_features = MODELS[model_type]\n",
    "    base_model = create_function(pretrained=True)\n",
    "    # base_model = create_function(pretrained=True, aux_logits=False)\n",
    "    # base_model = create_function(num_classes=1000, pretrained='imagenet')\n",
    "    # def identity(x): return x\n",
    "    # base_model.logits = identity\n",
    "    base_model = nn.Sequential(*list(base_model.children())[:-1])\n",
    "    return NIMA(base_model=base_model, input_features=input_features, drop_out=drop_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "332db142-2cc3-4e75-9d4b-e63df9a6dcbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import time\n",
    "from pathlib import Path\n",
    "from typing import Tuple\n",
    "\n",
    "import torch\n",
    "import torch.optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tqdm import tqdm\n",
    "# logger = logging.getLogger(__file__)\n",
    "def get_dataloaders(\n",
    "    path_to_save_csv: Path, path_to_images: Path, batch_size: int, num_workers: int\n",
    ") -> Tuple[DataLoader, DataLoader, DataLoader]:\n",
    "    transform = Transform()\n",
    "\n",
    "    train_ds = AVADataset(f\"{path_to_save_csv}/train.csv\", path_to_images, transform.train_transform)\n",
    "    val_ds = AVADataset(f\"{path_to_save_csv}/val.csv\", path_to_images, transform.val_transform)\n",
    "    test_ds = AVADataset(f\"{path_to_save_csv}/test.csv\", path_to_images, transform.val_transform)\n",
    "\n",
    "    train_loader = DataLoader(train_ds, batch_size=batch_size, num_workers=num_workers, shuffle=True)\n",
    "    val_loader = DataLoader(val_ds, batch_size=batch_size, num_workers=num_workers, shuffle=False)\n",
    "    test_ds = DataLoader(test_ds, batch_size=batch_size, num_workers=num_workers, shuffle=False)\n",
    "    return train_loader, val_loader, test_ds\n",
    "\n",
    "\n",
    "def validate_and_test(\n",
    "    path_to_save_csv: Path,\n",
    "    path_to_images: Path,\n",
    "    batch_size: int,\n",
    "    num_workers: int,\n",
    "    drop_out: float,\n",
    "    path_to_model_state: Path,\n",
    ") -> None:\n",
    "    _, val_loader, test_loader = get_dataloaders(\n",
    "        path_to_save_csv=path_to_save_csv, path_to_images=path_to_images, batch_size=batch_size, num_workers=num_workers\n",
    "    )\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    criterion = EDMLoss().to(device)\n",
    "\n",
    "    best_state = torch.load(path_to_model_state)\n",
    "\n",
    "    model = create_model(best_state[\"model_type\"], drop_out=drop_out).to(device)\n",
    "    model.load_state_dict(best_state[\"state_dict\"])\n",
    "\n",
    "    model.eval()\n",
    "    validate_losses = AverageMeter()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for (x, y) in tqdm(val_loader):\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            y_pred = model(x)\n",
    "            loss = criterion(p_target=y, p_estimate=y_pred)\n",
    "            validate_losses.update(loss.item(), x.size(0))\n",
    "\n",
    "    test_losses = AverageMeter()\n",
    "    with torch.no_grad():\n",
    "        for (x, y) in tqdm(test_loader):\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            y_pred = model(x)\n",
    "            loss = criterion(p_target=y, p_estimate=y_pred)\n",
    "            test_losses.update(loss.item(), x.size(0))\n",
    "   \n",
    "    print(f\"val loss {validate_losses.avg}; test loss {test_losses.avg}\")\n",
    "    # logger.info(f\"val loss {validate_losses.avg}; test loss {test_losses.avg}\")\n",
    "\n",
    "\n",
    "def get_optimizer(optimizer_type: str, model: NIMA, init_lr: float) -> torch.optim.Optimizer:\n",
    "    if optimizer_type == \"adam\":\n",
    "        # optimizer = torch.optim.Adam(model.parameters(), lr=init_lr)\n",
    "        optimizer = torch.optim.Adam([\n",
    "                {'params': model.base_model.parameters(), 'lr': init_lr * 0.1},\n",
    "                {'params': model.head.parameters(), 'lr': init_lr}\n",
    "            ])\n",
    "    elif optimizer_type == \"sgd\":\n",
    "        optimizer = torch.optim.SGD(model.parameters(), lr=init_lr, momentum=0.5, weight_decay=9)\n",
    "    else:\n",
    "        raise ValueError(f\"not such optimizer {optimizer_type}\")\n",
    "    return optimizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f297ac63-c085-4ce4-b213-e30a0401b432",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class Trainer:\n",
    "    def __init__(\n",
    "        self,\n",
    "        *,\n",
    "        path_to_save_csv: Path,\n",
    "        path_to_images: Path,\n",
    "        num_epoch: int,\n",
    "        model_type: str,\n",
    "        model_pth: Path,\n",
    "        num_workers: int,\n",
    "        batch_size: int,\n",
    "        init_lr: float,\n",
    "        experiment_dir: Path,\n",
    "        drop_out: float,\n",
    "        optimizer_type: str,\n",
    "    ):\n",
    "\n",
    "        train_loader, val_loader, test_loader = get_dataloaders(\n",
    "            path_to_save_csv=path_to_save_csv,\n",
    "            path_to_images=path_to_images,\n",
    "            batch_size=batch_size,\n",
    "            num_workers=num_workers,\n",
    "        )\n",
    "        self.train_loader = train_loader\n",
    "        self.val_loader = val_loader\n",
    "        self.test_loader = test_loader\n",
    "\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        model = create_model(model_type, drop_out=drop_out).to(self.device)\n",
    "        # print(model)\n",
    "\n",
    "        # load pretrained parameter\n",
    "        if model_pth:\n",
    "            state = torch.load(model_pth, map_location=torch.device(self.device))\n",
    "            model.load_state_dict(state[\"state_dict\"])\n",
    "            model.eval()\n",
    "        \n",
    "\n",
    "        optimizer = get_optimizer(optimizer_type=optimizer_type, model=model, init_lr=init_lr)\n",
    "\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "\n",
    "        # self.scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer=self.optimizer, mode=\"min\", patience=5)\n",
    "        self.scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.95)\n",
    "\n",
    "        self.criterion = EDMLoss().to(self.device)\n",
    "        self.binary_loss = BinaryLoss().to(self.device)\n",
    "        self.binary_loss5 = BinaryLoss5().to(self.device)\n",
    "        \n",
    "        self.model_type = model_type\n",
    "\n",
    "        # experiment_dir.mkdir(exist_ok=True, parents=True)\n",
    "        self.experiment_dir = experiment_dir\n",
    "        self.writer = SummaryWriter(f\"{experiment_dir}/logs\")\n",
    "        self.print_freq = 300\n",
    "        self.num_epoch = num_epoch\n",
    "\n",
    "    def train_model(self):\n",
    "        best_loss = float(\"inf\")\n",
    "        best_state = None\n",
    "        for e in range(1, self.num_epoch + 1):\n",
    "            \n",
    "            train_loss = self.train()\n",
    "            val_loss = self.validate()\n",
    "            binary_loss = self.binary_classify()\n",
    "            binary_loss5 = self.binary_classify5()\n",
    "            \n",
    "            # self.scheduler.step(metrics=val_loss)\n",
    "            self.scheduler.step()\n",
    "\n",
    "            self.writer.add_scalar(\"train/loss\", train_loss, global_step=e)\n",
    "            self.writer.add_scalar(\"val/loss\", val_loss, global_step=e)\n",
    "            print(f\"updated loss from {best_loss} to {val_loss}\")\n",
    "            logger.info(f\"updated loss from {best_loss} to {val_loss}\")\n",
    "            \n",
    "\n",
    "            current_state = {\n",
    "                \"state_dict\": self.model.state_dict(),\n",
    "                \"model_type\": self.model_type,\n",
    "                \"epoch\": e,\n",
    "                \"val_loss\": val_loss,\n",
    "            }\n",
    "            torch.save(current_state, f\"{self.experiment_dir}/{self.model_type}_epoch{e}_new.pth\")\n",
    "            \n",
    "            if best_state is None or val_loss < best_loss:\n",
    "                # logger.info(f\"updated loss from {best_loss} to {val_loss}\")\n",
    "                \n",
    "                best_loss = val_loss\n",
    "                best_state = {\n",
    "                    \"state_dict\": self.model.state_dict(),\n",
    "                    \"model_type\": self.model_type,\n",
    "                    \"epoch\": e,\n",
    "                    \"best_loss\": best_loss,\n",
    "                }\n",
    "                torch.save(best_state, f\"{self.experiment_dir}/{self.model_type}_best_state_epoch{e}.pth\")\n",
    "            \n",
    "            logger.info(f\"Binary classification accuracy is {binary_loss}\")\n",
    "            logger.info(f\"Binary classification accuracy 5.5 is {binary_loss5}\")\n",
    "            \n",
    "\n",
    "    def train(self):\n",
    "        self.model.train()\n",
    "        train_losses = AverageMeter()\n",
    "        total_iter = len(self.train_loader.dataset) // self.train_loader.batch_size\n",
    "        total_loss = 0\n",
    "\n",
    "        for idx, (x, y) in enumerate(self.train_loader):\n",
    "            s = time.monotonic()\n",
    "\n",
    "            x = x.to(self.device)\n",
    "            y = y.to(self.device)\n",
    "            y_pred = self.model(x)\n",
    "            loss = self.criterion(p_target=y, p_estimate=y_pred)\n",
    "            self.optimizer.zero_grad()\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            self.optimizer.step()\n",
    "            train_losses.update(loss.item(), x.size(0))\n",
    "\n",
    "\n",
    "            e = time.monotonic()\n",
    "            total_loss += loss\n",
    "            if not (idx + 1) % self.print_freq:\n",
    "                \n",
    "                log_time = self.print_freq * (e - s)\n",
    "                eta = ((total_iter - idx) * log_time) / 60.0\n",
    "                logger.info(f\"iter #[{idx}/{total_iter}] \" f\"loss = {total_loss/self.print_freq:.3f} \" f\"time = {log_time:.2f} \" f\"eta = {eta:.2f}\")\n",
    "                total_loss = 0\n",
    "\n",
    "        return train_losses.avg\n",
    "\n",
    "    def validate(self):\n",
    "        self.model.eval()\n",
    "        validate_losses = AverageMeter()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for idx, (x, y) in enumerate(self.val_loader):\n",
    "                x = x.to(self.device)\n",
    "                y = y.to(self.device)\n",
    "                y_pred = self.model(x)\n",
    "                loss = self.criterion(p_target=y, p_estimate=y_pred)\n",
    "                validate_losses.update(loss.item(), x.size(0))\n",
    "\n",
    "        return validate_losses.avg\n",
    "\n",
    "    def binary_classify(self):\n",
    "        self.model.eval()\n",
    "        binary_losses = AverageMeter()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for (x, y) in tqdm(self.test_loader):\n",
    "                x = x.to(self.device)\n",
    "                y = y.to(self.device)\n",
    "                y_pred = self.model(x)\n",
    "                loss = self.binary_loss(p_target=y, p_estimate=y_pred)\n",
    "                binary_losses.update(loss, x.size(0))\n",
    "        return binary_losses.avg\n",
    "        \n",
    "        \n",
    "    def binary_classify5(self):\n",
    "        self.model.eval()\n",
    "        binary_losses = AverageMeter()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for (x, y) in tqdm(self.test_loader):\n",
    "                x = x.to(self.device)\n",
    "                y = y.to(self.device)\n",
    "                y_pred = self.model(x)\n",
    "                loss = self.binary_loss5(p_target=y, p_estimate=y_pred)\n",
    "                binary_losses.update(loss, x.size(0))\n",
    "        return binary_losses.avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "74760685-8675-4cba-9f43-2920ca088b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(\n",
    "    path_to_save_csv: Path,\n",
    "    path_to_images: Path,\n",
    "    experiment_dir: Path,\n",
    "    model_type: str,\n",
    "    model_pth: Path,\n",
    "    batch_size: int,\n",
    "    num_workers: int,\n",
    "    num_epoch: int,\n",
    "    init_lr: float,\n",
    "    drop_out: float,\n",
    "    optimizer_type: str,\n",
    "    seed: int,\n",
    "):\n",
    "    logger.info(f'''\n",
    "    path_to_save_csv:{path_to_save_csv},\n",
    "    path_to_images:{path_to_images},\n",
    "    experiment_dir:{experiment_dir},\n",
    "    model_type:{model_type},\n",
    "    model_pth:{model_pth},\n",
    "    batch_size:{batch_size},\n",
    "    num_workers:{num_workers},\n",
    "    num_epoch:{num_epoch},\n",
    "    init_lr:{init_lr},\n",
    "    drop_out:{drop_out},\n",
    "    optimizer_type:{optimizer_type},\n",
    "    seed:{seed},\n",
    "    ''')\n",
    "\n",
    "    logger.info(\"Train and validate model\")\n",
    "    set_up_seed(seed)\n",
    "    trainer = Trainer(\n",
    "        path_to_save_csv=path_to_save_csv,\n",
    "        path_to_images=path_to_images,\n",
    "        experiment_dir=experiment_dir,\n",
    "        model_type=model_type,\n",
    "        model_pth=model_pth,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=num_workers,\n",
    "        num_epoch=num_epoch,\n",
    "        init_lr=init_lr,\n",
    "        drop_out=drop_out,\n",
    "        optimizer_type=optimizer_type,\n",
    "    )\n",
    "    trainer.train_model()\n",
    "    logger.info(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "851ef5d3-8066-4e8b-ba36-4c7f43c1e479",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def get_mean_score(score):\n",
    "    buckets = torch.arange(1, 11).to(device)\n",
    "    mu = torch.sum(buckets * score, 1)\n",
    "    return mu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fa4e7c0f-044f-4e15-857f-d282d8a0bb6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import sys\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "formatter = logging.Formatter('%(asctime)s | %(levelname)s | %(message)s')\n",
    "\n",
    "\n",
    "file_handler = logging.FileHandler('***.log', mode='w')\n",
    "file_handler.setLevel(logging.INFO)\n",
    "file_handler.setFormatter(formatter)\n",
    "\n",
    "logger.addHandler(file_handler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2203484-cef5-4328-b52d-e255fa39de52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "train_model(path_to_save_csv = \".../dataset_csv\",\n",
    "    path_to_images = \"/.../image\",\n",
    "    experiment_dir = \".../result/\",\n",
    "    model_type = \"mobilenet-v3-small\",\n",
    "    model_pth = \"\",\n",
    "    batch_size = 128,\n",
    "    num_workers = 4,\n",
    "    num_epoch = 64,\n",
    "    # init_lr = 0.0001,\n",
    "    # init_lr = 0.000003,\n",
    "    init_lr = 0.0001,\n",
    "    # drop_out = 0.5,\n",
    "    drop_out = 0.75,\n",
    "    optimizer_type = \"adam\",\n",
    "    seed = 9,)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "917cc881-f6ea-419e-a179-069cf60db8d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a95a3d6-89d7-4049-8f15-1022dfa8c34a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
